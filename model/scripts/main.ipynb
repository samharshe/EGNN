{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "import os\n",
    "from EGNN5 import EGNN5\n",
    "from MD17_data import benzene_dataloaders\n",
    "import wandb\n",
    "from test_model import test_model\n",
    "from train_model import train_model\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ba8ffd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproducibility\n",
    "torch.manual_seed(2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters saved to config dict\n",
    "config = {\n",
    "    'name': 'EGNN5',\n",
    "    'base_learning_rate': 0.001,\n",
    "    'num_epochs': 5,\n",
    "    'optimizer': 'Adam',\n",
    "    'scheduler': 'ReduceLROnPlateau',\n",
    "    'scheduler_mode': 'min',\n",
    "    'scheduler_factor': 0.32, \n",
    "    'scheduler_patience': 1,\n",
    "    'scheduler_threshold': 0,\n",
    "    'training_loss_fn': 'MSELoss',\n",
    "    'rho': 1-1e-1,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "# initialize the star of the show\n",
    "model = EGNN5()\n",
    "\n",
    "# I couldn't think of a concise way to initialize optimizer, scheduler, and loss_fn based on the contents of config\n",
    "# this is all for show anyway, but it would be nice to have a natural way of doing this that generalizes when I am selecting hyperparameters more carefully\n",
    "optimizer = Adam(model.parameters(), lr=config['base_learning_rate'])\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer=optimizer, \n",
    "    mode=config['scheduler_mode'], \n",
    "    factor=config['scheduler_factor'], \n",
    "    patience=config['scheduler_patience'], \n",
    "    threshold=config['scheduler_threshold']\n",
    "    )\n",
    "\n",
    "loss_fn = MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/samharshe/Documents/Gerstein Lab/EGNN/model/scripts/wandb/run-20240621_120133-2vdo5ijt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sharshe/EGNN/runs/2vdo5ijt' target=\"_blank\">fanciful-dust-35</a></strong> to <a href='https://wandb.ai/sharshe/EGNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sharshe/EGNN' target=\"_blank\">https://wandb.ai/sharshe/EGNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sharshe/EGNN/runs/2vdo5ijt' target=\"_blank\">https://wandb.ai/sharshe/EGNN/runs/2vdo5ijt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sharshe/EGNN/runs/2vdo5ijt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x124416190>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up wandb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'main.py'\n",
    "\n",
    "# wandb\n",
    "wandb.init(\n",
    "    project = \"EGNN\",\n",
    "    config = config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch_geometric/data/dataset.py:239: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, pass `force_reload=True` explicitly to reload the dataset.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# get dataloaders\n",
    "train_loader, val_loader, test_loader = benzene_dataloaders(train_split=0.8, val_split=0.1, test_split = 0.1, batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 OF 5 | VAL MEAN LOSS: 3.7275174236128805e-07\n",
      "EPOCH 2 OF 5 | VAL MEAN LOSS: 1.4036270385986427e-06\n",
      "EPOCH 3 OF 5 | VAL MEAN LOSS: 8.260968797912938e-08\n",
      "EPOCH 4 OF 5 | VAL MEAN LOSS: 7.601409635071832e-08\n",
      "EPOCH 5 OF 5 | VAL MEAN LOSS: 1.5135384501263616e-08\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'E_squared_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_model(model\u001b[38;5;241m=\u001b[39mmodel, optimizer\u001b[38;5;241m=\u001b[39moptimizer, scheduler\u001b[38;5;241m=\u001b[39mscheduler, loss_fn\u001b[38;5;241m=\u001b[39mloss_fn, train_loader\u001b[38;5;241m=\u001b[39mtrain_loader, val_loader\u001b[38;5;241m=\u001b[39mval_loader, rho\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrho\u001b[39m\u001b[38;5;124m'\u001b[39m], num_epochs\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m], name\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrho\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Gerstein Lab/EGNN/model/scripts/test_model.py:46\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, loss_fn, test_loader, rho)\u001b[0m\n\u001b[1;32m     43\u001b[0m F_loss \u001b[38;5;241m=\u001b[39m rho \u001b[38;5;241m*\u001b[39m F_loss_fn(F_hat, F, loss_fn)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# canonical loss\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mE_squared_loss\u001b[49m \u001b[38;5;241m+\u001b[39m F_squared_loss\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# absolute error for energy loss\u001b[39;00m\n\u001b[1;32m     49\u001b[0m E_absolute_loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m rho) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mabs(E_hat\u001b[38;5;241m-\u001b[39mE))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'E_squared_loss' is not defined"
     ]
    }
   ],
   "source": [
    "train_model(model=model, optimizer=optimizer, scheduler=scheduler, loss_fn=loss_fn, train_loader=train_loader, val_loader=val_loader, rho=config['rho'], num_epochs=config['num_epochs'], name=config['name'])\n",
    "test_model(model=model, loss_fn=loss_fn, test_loader=test_loader, rho=config['rho'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
